{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 11:29:40.011676: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "import keras.layers as layers\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "from db_actions import DatabaseActions as db\n",
    "import tokenizing\n",
    "from frequency_analysis import get_prob\n",
    "from structure.train_data import Record, StoredData\n",
    "\n",
    "\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_len_all:  222\n",
      "fake_len_train:  155\n",
      "fake_len_test:  67\n",
      "not_fake_len_all:  1751\n",
      "not_fake_len_train:  533\n",
      "not_fake_len_test:  67\n"
     ]
    }
   ],
   "source": [
    "data = db.get_parsed()\n",
    "\n",
    "storedData = StoredData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 768)\n",
      "<class 'numpy.ndarray'> 768\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.ndarray'> 688\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(storedData.x_train.shape)\n",
    "print(type(storedData.x_train[0]), len(storedData.x_train[0]))\n",
    "print(type(storedData.x_train[0][0]))\n",
    "print(type(storedData.y_train), len(storedData.y_train))\n",
    "print(type(storedData.y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(768 * 16, activation='elu', input_shape=(768,)))\n",
    "# model.add(Dense(1024 * 8, activation='elu'))\n",
    "# model.add(Dense(1024 * 4, activation='elu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(1024 * 4, activation='elu'))\n",
    "# model.add(Dense(1024, activation='elu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(512, activation='elu'))\n",
    "# model.add(Dense(256, activation='elu'))\n",
    "# model.add(Dense(256, activation='elu'))\n",
    "# model.add(Dense(64, activation='elu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(curve='ROC')])\n",
    "\n",
    "\n",
    "# # model.summary()\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(storedData.x_train, storedData.y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = storedData.x_train.reshape((len(storedData.x_train), 24, 32))\n",
    "x_ts = storedData.x_test.reshape((len(storedData.x_test), 24, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# Create an instance of the model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m input_shape \u001b[39m=\u001b[39m (\u001b[39m24\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# Example shape, adjust based on your data\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m model \u001b[39m=\u001b[39m create_cnn_model(input_shape)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9),\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m#               loss='binary_crossentropy',\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m#               metrics=[tf.keras.metrics.AUC(curve='ROC')])\u001b[39;00m\n\u001b[1;32m     47\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m               metrics\u001b[39m=\u001b[39m[ tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mAUC(curve\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mROC\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     50\u001b[0m                         keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mPrecision(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     51\u001b[0m                         keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mRecall(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     52\u001b[0m                         keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mBinaryAccuracy(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)])\n",
      "Cell \u001b[0;32mIn[75], line 24\u001b[0m, in \u001b[0;36mcreate_cnn_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Fully connected layer with 64 units and ReLU activation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDense(\u001b[39m256\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m model\u001b[39m.\u001b[39madd(layes\u001b[39m.\u001b[39mDropout(\u001b[39m0.2\u001b[39m))\n\u001b[1;32m     25\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[39m# Output layer with 1 unit and sigmoid activation for binary classification\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layes' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation\n",
    "    model.add(layers.Conv2D(256, (9, 9), activation='tanh', input_shape=input_shape))\n",
    "    \n",
    "    # Max pooling layer with 2x2 pool size\n",
    "    model.add(layers.MaxPooling2D((3, 3)))\n",
    "    \n",
    "    # Convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation\n",
    "    model.add(layers.Conv2D(512, (4, 4), activation='tanh'))\n",
    "    \n",
    "    # Max pooling layer with 2x2 pool size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flatten the output from the previous layer\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Fully connected layer with 64 units and ReLU activation\n",
    "    model.add(layers.Dense(256, activation='tanh'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation='tanh'))\n",
    "    \n",
    "    # Output layer with 1 unit and sigmoid activation for binary classification\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Create an instance of the model\n",
    "input_shape = (24, 32, 1) # Example shape, adjust based on your data\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(curve='ROC')])\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[ tf.keras.metrics.AUC(curve='ROC'),\n",
    "                        keras.metrics.Precision(name='precision'),\n",
    "                        keras.metrics.Recall(name='recall'),\n",
    "                        keras.metrics.BinaryAccuracy(name='accuracy')])\n",
    "                                            \n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "22/22 [==============================] - 3s 114ms/step - loss: 0.5889 - auc_16: 0.5599\n",
      "Epoch 2/25\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.4684 - auc_16: 0.7568\n",
      "Epoch 3/25\n",
      "22/22 [==============================] - 3s 114ms/step - loss: 0.4308 - auc_16: 0.7853\n",
      "Epoch 4/25\n",
      "22/22 [==============================] - 2s 106ms/step - loss: 0.4227 - auc_16: 0.7985\n",
      "Epoch 5/25\n",
      "22/22 [==============================] - 2s 108ms/step - loss: 0.4099 - auc_16: 0.8071\n",
      "Epoch 6/25\n",
      "22/22 [==============================] - 2s 113ms/step - loss: 0.4082 - auc_16: 0.8190\n",
      "Epoch 7/25\n",
      "22/22 [==============================] - 3s 113ms/step - loss: 0.4208 - auc_16: 0.7975\n",
      "Epoch 8/25\n",
      "22/22 [==============================] - 3s 117ms/step - loss: 0.4042 - auc_16: 0.8166\n",
      "Epoch 9/25\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.4076 - auc_16: 0.8197\n",
      "Epoch 10/25\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.4010 - auc_16: 0.8072\n",
      "Epoch 11/25\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.4029 - auc_16: 0.8118\n",
      "Epoch 12/25\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.4050 - auc_16: 0.8003\n",
      "Epoch 13/25\n",
      "22/22 [==============================] - 2s 112ms/step - loss: 0.4092 - auc_16: 0.8057\n",
      "Epoch 14/25\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.4146 - auc_16: 0.8092\n",
      "Epoch 15/25\n",
      "22/22 [==============================] - 2s 112ms/step - loss: 0.4072 - auc_16: 0.8052\n",
      "Epoch 16/25\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.3992 - auc_16: 0.8213\n",
      "Epoch 17/25\n",
      "22/22 [==============================] - 3s 132ms/step - loss: 0.3996 - auc_16: 0.8160\n",
      "Epoch 18/25\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.3987 - auc_16: 0.8221\n",
      "Epoch 19/25\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.4035 - auc_16: 0.8197\n",
      "Epoch 20/25\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.4041 - auc_16: 0.8098\n",
      "Epoch 21/25\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.3955 - auc_16: 0.8213\n",
      "Epoch 22/25\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.3947 - auc_16: 0.8194\n",
      "Epoch 23/25\n",
      "22/22 [==============================] - 3s 115ms/step - loss: 0.3936 - auc_16: 0.8357\n",
      "Epoch 24/25\n",
      "22/22 [==============================] - 3s 115ms/step - loss: 0.4003 - auc_16: 0.8277\n",
      "Epoch 25/25\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.3980 - auc_16: 0.8225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16aae5a90>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_t, storedData.y_train, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step\n",
      "0.2952615538639809\n",
      "[0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0\n",
      " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0\n",
      " 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1]\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "NO [False] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "NO [ True] 0\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "NO [False] 1\n",
      "NO [ True] 0\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "YES [ True] 1\n",
      "NO [ True] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "YES [False] 0\n",
      "YES [False] 0\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "NO [False] 1\n",
      "NO [ True] 0\n",
      "YES [False] 0\n",
      "NO [False] 1\n",
      "YES [ True] 1\n",
      "NO [False] 1\n",
      "101 134\n",
      "0.753731343283582\n"
     ]
    }
   ],
   "source": [
    "# for i in range(95, 105):\n",
    "#     print(i, y_train[i], texts[i], sep= '\\t', end='\\t')\n",
    "#     print()\n",
    "\n",
    "\n",
    "res = model.predict(x_ts)\n",
    "dif = np.sum(res) / np.size(res)\n",
    "\n",
    "print(dif)\n",
    "print(storedData.y_test)\n",
    "c = 0\n",
    "for i in range(len(x_ts)):\n",
    "    if (res[i] > dif) == storedData.y_test[i]:\n",
    "        print(\"YES\", res[i] > dif, storedData.y_test[i])\n",
    "        c += 1\n",
    "    else:\n",
    "        print(\"NO\", res[i] > dif, storedData.y_test[i])\n",
    "print(c, len(storedData.x_test))\n",
    "print(c / len(storedData.x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\t0\t) \t[0.01977045]\t[False]\t0\n",
      "(\t1\t) \t[0.49628803]\t[ True]\t1\n",
      "(\t2\t) \t[0.02641177]\t[False]\t1\n",
      "(\t3\t) \t[0.49573264]\t[ True]\t0\n",
      "(\t4\t) \t[0.04678861]\t[False]\t0\n",
      "(\t5\t) \t[0.49615267]\t[ True]\t1\n",
      "(\t6\t) \t[0.1799083]\t[False]\t0\n",
      "(\t7\t) \t[0.03508859]\t[False]\t1\n",
      "(\t8\t) \t[0.01831624]\t[False]\t0\n",
      "(\t9\t) \t[0.48263407]\t[ True]\t1\n",
      "(\t10\t) \t[0.03025738]\t[False]\t0\n",
      "(\t11\t) \t[0.49625877]\t[ True]\t0\n",
      "(\t12\t) \t[0.07379356]\t[False]\t0\n",
      "(\t13\t) \t[0.18321289]\t[False]\t0\n",
      "(\t14\t) \t[0.18196613]\t[False]\t0\n",
      "(\t15\t) \t[0.4843428]\t[ True]\t1\n",
      "(\t16\t) \t[0.495828]\t[ True]\t1\n",
      "(\t17\t) \t[0.49579364]\t[ True]\t1\n",
      "(\t18\t) \t[0.495663]\t[ True]\t1\n",
      "(\t19\t) \t[0.47133666]\t[ True]\t0\n",
      "(\t20\t) \t[0.18270022]\t[False]\t0\n",
      "(\t21\t) \t[0.18251453]\t[False]\t0\n",
      "(\t22\t) \t[0.49639526]\t[ True]\t1\n",
      "(\t23\t) \t[0.49631646]\t[ True]\t1\n",
      "(\t24\t) \t[0.01628275]\t[False]\t0\n",
      "(\t25\t) \t[0.01643713]\t[False]\t0\n",
      "(\t26\t) \t[0.4814355]\t[ True]\t1\n",
      "(\t27\t) \t[0.06743386]\t[False]\t0\n",
      "(\t28\t) \t[0.479601]\t[ True]\t1\n",
      "(\t29\t) \t[0.4964132]\t[ True]\t0\n",
      "(\t30\t) \t[0.49594477]\t[ True]\t0\n",
      "(\t31\t) \t[0.05703238]\t[False]\t0\n",
      "(\t32\t) \t[0.496242]\t[ True]\t1\n",
      "(\t33\t) \t[0.4958672]\t[ True]\t1\n",
      "(\t34\t) \t[0.18084057]\t[False]\t1\n",
      "(\t35\t) \t[0.4957065]\t[ True]\t1\n",
      "(\t36\t) \t[0.0411915]\t[False]\t0\n",
      "(\t37\t) \t[0.49602965]\t[ True]\t1\n",
      "(\t38\t) \t[0.49626336]\t[ True]\t1\n",
      "(\t39\t) \t[0.01578167]\t[False]\t0\n",
      "(\t40\t) \t[0.18302906]\t[False]\t0\n",
      "(\t41\t) \t[0.49600264]\t[ True]\t1\n",
      "(\t42\t) \t[0.18070869]\t[False]\t1\n",
      "(\t43\t) \t[0.4960471]\t[ True]\t1\n",
      "(\t44\t) \t[0.4951135]\t[ True]\t1\n",
      "(\t45\t) \t[0.4954044]\t[ True]\t1\n",
      "(\t46\t) \t[0.01607068]\t[False]\t0\n",
      "(\t47\t) \t[0.4959501]\t[ True]\t1\n",
      "(\t48\t) \t[0.17626086]\t[False]\t0\n",
      "(\t49\t) \t[0.49586314]\t[ True]\t1\n",
      "(\t50\t) \t[0.06550144]\t[False]\t1\n",
      "(\t51\t) \t[0.4958147]\t[ True]\t1\n",
      "(\t52\t) \t[0.05285781]\t[False]\t1\n",
      "(\t53\t) \t[0.47115317]\t[ True]\t1\n",
      "(\t54\t) \t[0.49599156]\t[ True]\t1\n",
      "(\t55\t) \t[0.49630493]\t[ True]\t0\n",
      "(\t56\t) \t[0.49633697]\t[ True]\t1\n",
      "(\t57\t) \t[0.49491554]\t[ True]\t0\n",
      "(\t58\t) \t[0.49557704]\t[ True]\t0\n",
      "(\t59\t) \t[0.4963816]\t[ True]\t1\n",
      "(\t60\t) \t[0.1904241]\t[False]\t1\n",
      "(\t61\t) \t[0.18074286]\t[False]\t0\n",
      "(\t62\t) \t[0.0173643]\t[False]\t0\n",
      "(\t63\t) \t[0.18120629]\t[False]\t1\n",
      "(\t64\t) \t[0.44376966]\t[ True]\t1\n",
      "(\t65\t) \t[0.05952759]\t[False]\t0\n",
      "(\t66\t) \t[0.49370217]\t[ True]\t1\n",
      "(\t67\t) \t[0.01603439]\t[False]\t0\n",
      "(\t68\t) \t[0.49607593]\t[ True]\t0\n",
      "(\t69\t) \t[0.18257116]\t[False]\t0\n",
      "(\t70\t) \t[0.01605328]\t[False]\t0\n",
      "(\t71\t) \t[0.18098286]\t[False]\t1\n",
      "(\t72\t) \t[0.49599835]\t[ True]\t1\n",
      "(\t73\t) \t[0.07801379]\t[False]\t0\n",
      "(\t74\t) \t[0.03811043]\t[False]\t0\n",
      "(\t75\t) \t[0.01608517]\t[False]\t0\n",
      "(\t76\t) \t[0.01956902]\t[False]\t0\n",
      "(\t77\t) \t[0.03850843]\t[False]\t0\n",
      "(\t78\t) \t[0.49600458]\t[ True]\t1\n",
      "(\t79\t) \t[0.01586315]\t[False]\t0\n",
      "(\t80\t) \t[0.0243034]\t[False]\t0\n",
      "(\t81\t) \t[0.4957733]\t[ True]\t1\n",
      "(\t82\t) \t[0.49625665]\t[ True]\t0\n",
      "(\t83\t) \t[0.49630958]\t[ True]\t1\n",
      "(\t84\t) \t[0.4959514]\t[ True]\t0\n",
      "(\t85\t) \t[0.06148257]\t[False]\t1\n",
      "(\t86\t) \t[0.4949252]\t[ True]\t0\n",
      "(\t87\t) \t[0.49637637]\t[ True]\t0\n",
      "(\t88\t) \t[0.18291232]\t[False]\t0\n",
      "(\t89\t) \t[0.01710682]\t[False]\t0\n",
      "(\t90\t) \t[0.08112067]\t[False]\t0\n",
      "(\t91\t) \t[0.07652679]\t[False]\t0\n",
      "(\t92\t) \t[0.18173778]\t[False]\t0\n",
      "(\t93\t) \t[0.4963714]\t[ True]\t1\n",
      "(\t94\t) \t[0.18187337]\t[False]\t0\n",
      "(\t95\t) \t[0.4951101]\t[ True]\t1\n",
      "(\t96\t) \t[0.49562776]\t[ True]\t0\n",
      "(\t97\t) \t[0.49371237]\t[ True]\t0\n",
      "(\t98\t) \t[0.01919974]\t[False]\t0\n",
      "(\t99\t) \t[0.4960761]\t[ True]\t1\n",
      "(\t100\t) \t[0.49606556]\t[ True]\t1\n",
      "(\t101\t) \t[0.49619976]\t[ True]\t1\n",
      "(\t102\t) \t[0.18242657]\t[False]\t0\n",
      "(\t103\t) \t[0.49575442]\t[ True]\t1\n",
      "(\t104\t) \t[0.17979641]\t[False]\t1\n",
      "(\t105\t) \t[0.1797862]\t[False]\t0\n",
      "(\t106\t) \t[0.044482]\t[False]\t0\n",
      "(\t107\t) \t[0.49590427]\t[ True]\t1\n",
      "(\t108\t) \t[0.49596974]\t[ True]\t1\n",
      "(\t109\t) \t[0.49601638]\t[ True]\t1\n",
      "(\t110\t) \t[0.01905232]\t[False]\t0\n",
      "(\t111\t) \t[0.18139055]\t[False]\t0\n",
      "(\t112\t) \t[0.01568919]\t[False]\t0\n",
      "(\t113\t) \t[0.49602816]\t[ True]\t1\n",
      "(\t114\t) \t[0.49583933]\t[ True]\t1\n",
      "(\t115\t) \t[0.4957172]\t[ True]\t1\n",
      "(\t116\t) \t[0.06151938]\t[False]\t0\n",
      "(\t117\t) \t[0.18297093]\t[False]\t1\n",
      "(\t118\t) \t[0.47895107]\t[ True]\t1\n",
      "(\t119\t) \t[0.4959843]\t[ True]\t1\n",
      "(\t120\t) \t[0.48335764]\t[ True]\t0\n",
      "(\t121\t) \t[0.496364]\t[ True]\t1\n",
      "(\t122\t) \t[0.01672847]\t[False]\t0\n",
      "(\t123\t) \t[0.49571753]\t[ True]\t1\n",
      "(\t124\t) \t[0.0218248]\t[False]\t0\n",
      "(\t125\t) \t[0.01604821]\t[False]\t0\n",
      "(\t126\t) \t[0.49540958]\t[ True]\t1\n",
      "(\t127\t) \t[0.18099974]\t[False]\t1\n",
      "(\t128\t) \t[0.18096761]\t[False]\t1\n",
      "(\t129\t) \t[0.48176715]\t[ True]\t0\n",
      "(\t130\t) \t[0.05895426]\t[False]\t0\n",
      "(\t131\t) \t[0.18266512]\t[False]\t1\n",
      "(\t132\t) \t[0.4955572]\t[ True]\t1\n",
      "(\t133\t) \t[0.05452289]\t[False]\t1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(storedData.x_test)):\n",
    "    print('(', i, ') ', res[i], res[i] > dif, storedData.y_test[i], sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 15ms/step\n",
      "0.23183946831281796\n",
      "529 688\n",
      "0.7688953488372093\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(x_t)\n",
    "dif2 = np.sum(res) / np.size(res)\n",
    "print(dif2)\n",
    "# 0.9999979073660714\n",
    "# 0.9999981471470424\n",
    "\n",
    "# 0.7628571428571429\n",
    "# 0.7628571428571429\n",
    "c = 0\n",
    "for i in range(len(x_t)):\n",
    "    if (res[i] > dif2) == storedData.y_train[i]:\n",
    "        c += 1\n",
    "print(c, len(x_t))\n",
    "print(c / len(x_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([2048, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([2048, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([2048, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([2048, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([2048, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "0.2952615538639809\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[[False]] [[0.19160731]]\n",
      "[[False]] [[0.18016975]]\n",
      "[[False]] [[0.22184788]]\n",
      "[[False]] [[0.15914516]]\n",
      "[[False]] [[0.19118807]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_review(text):\n",
    "    # vector = ((tokenizing.tokenize_res(text)) / 4 + 1) / 2\n",
    "    vector = tokenizing.tokenize_res(text) \n",
    "    print(vector.shape)\n",
    "    vector = vector.reshape((24, 32))\n",
    "    print('~' * 80)\n",
    "    # print(vector)\n",
    "    return(model.predict(np.array([vector,])))\n",
    "\n",
    "txt1 = \"Очень пнравился ресторан, кухня Азербайджанская, примлемые цены, все очень вкусное, чай превосходный, также на 2 этаже есть магазин с Азербайджанскими товарами ручной работы, в общем при посещении этого места, есть ощущение, что посетил Азербайджан\"\n",
    "txt2 = \"Превосходный ресторан! Друзья пригласили сюда, остался в восторге! Потрясающая кухня, приятная атмосфера и приветливый персонал. Так держать! Кстати, в туалете очень красивый дизайн\"\n",
    "txt3 = \"Наверно, станет одним из любимых мест. Вкусно очень! Я кулинар, поэтому оценила и качество продуктов, и тонкости приготовления. Здорово! Вернёмся. Рекомендую.  Звезду сняла за ОЧЕНЬ громкую и безвкусную (или очень на любителя) музыку.  Господа хозяева-менеджеры, вы прекрасно всё сделали, к вам ходят семьи с маленькими детьми, у вас неплохое детское меню. Не портите имидж, используйте приятную нейтральную музыку с нормальными децибеллами, серьёзный клиентский сектор отпугиваете!\"\n",
    "txt4 = \"мне очень понравился этот ресторан. очень классное место, приду еще\"\n",
    "txt5 = \"хорошо\"\n",
    "\n",
    "r1 = check_review(txt1)\n",
    "r2 = check_review(txt2)\n",
    "r3 = check_review(txt3)\n",
    "r4 = check_review(txt4)\n",
    "r5 = check_review(txt5)\n",
    "\n",
    "print(dif)\n",
    "print(\"~\" * 80)\n",
    "print(dif <= r1, r1)\n",
    "print(dif <= r2, r2)\n",
    "print(dif <= r3, r3)\n",
    "print(dif <= r4, r4)\n",
    "print(dif <= r5, r5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([2048, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[ True]] [[0.4132369]] 0.32472456747026585\n"
     ]
    }
   ],
   "source": [
    "tmp = check_review(\"идеальшейшая корчма -- она очень сильно понравится. для истинных гурманов\")\n",
    "\n",
    "print(dif <= tmp, tmp, dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# print(texts)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(texts)):\n\u001b[1;32m      7\u001b[0m     data\u001b[39m.\u001b[39mappend(get_prob(texts[i]))\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "# print(texts)\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    data.append(get_prob(texts[i]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d1 = np.array(data[0:100])\n",
    "d1.sort()\n",
    "# d1 = d1[0:90]\n",
    "\n",
    "d2 = np.array(data[100:200])\n",
    "d2.sort()\n",
    "\n",
    "# d2 = d2[0:90]\n",
    "# Create a line plot\n",
    "plt.plot(d1)\n",
    "plt.plot(d2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Array Data Visualization')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "print(\"fake\", np.median(d1), np.average(d1))\n",
    "print(\"normal\", np.median(d2), np.average(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr('raise') \n",
    "d3 = []\n",
    "s = 0.0\n",
    "for i in range(1, len(d1)):\n",
    "    print(d1[i], d2[i])\n",
    "    s = s + d1[i] / d2[i]\n",
    "    d3.append(s)\n",
    "print(d3)\n",
    "plt.plot(d3)\n",
    "# Add labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Array Data Visualization')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr('raise') \n",
    "d4 = []\n",
    "d1[:] = d1[::-1]\n",
    "d2[:] = d2[::-1]\n",
    "s = 0.0\n",
    "for i in range(1, len(d1)):\n",
    "    s = s + d1[i]\n",
    "    d4.append(s)\n",
    "\n",
    "plt.plot(d4)\n",
    "\n",
    "d5 = []\n",
    "s = 0.0\n",
    "for i in range(1, len(d1)):\n",
    "\n",
    "    s = s + d2[i]\n",
    "    d5.append(s)\n",
    "\n",
    "plt.plot(d5)\n",
    "# Add labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Array Data Visualization')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 19, 27, 128)       4736      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 6, 256)         524544    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 768)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                49216     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 578,561\n",
      "Trainable params: 578,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model-05062023/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model-05062023/assets\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# model.summary()\n",
    "# model.save('model-05062023')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
