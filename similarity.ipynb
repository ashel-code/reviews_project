{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_actions import DatabaseActions as db\n",
    "\n",
    "data = db.get_all()\n",
    "\n",
    "for i in range(len(data[0])):\n",
    "    print(i, data[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for r in data:\n",
    "    texts.append(r[4])\n",
    "\n",
    "print(texts[0], len(texts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quicksort_by_length(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    shorter = [text for text in arr if len(text) < len(pivot)]\n",
    "    equal = [text for text in arr if len(text) == len(pivot)]\n",
    "    longer = [text for text in arr if len(text) > len(pivot)]\n",
    "\n",
    "    return quicksort_by_length(shorter) + equal + quicksort_by_length(longer)\n",
    "\n",
    "# Example usage:\n",
    "sorted_texts = quicksort_by_length(texts)\n",
    "for text in sorted_texts:\n",
    "    print(text, end='\\n' + '~' * 80 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_config import write_list\n",
    "\n",
    "write_list(sorted_texts, \"./data/sorted_texts.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def levenshtein_distance(word1, word2):\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "\n",
    "    # Создаем матрицу размером (m+1) x (n+1)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Инициализируем первую строку и первый столбец\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Заполняем остальные ячейки матрицы\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if word1[i - 1] == word2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    words1 = sentence1.split()\n",
    "    words2 = sentence2.split()\n",
    "    similarity = 0.0\n",
    "\n",
    "    for word1 in words1:\n",
    "        word_similarity = 0.0\n",
    "        for word2 in words2:\n",
    "            word_distance = levenshtein_distance(word1, word2)\n",
    "            word_similarity = max(word_similarity, 1.0 - (word_distance / max(len(word1), len(word2))))\n",
    "        similarity += word_similarity\n",
    "\n",
    "    similarity /= len(words1)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "words1 = \"apple banana orange\"\n",
    "words2 = \"banana orange kiwi\"\n",
    "similarity = sentence_similarity(words1, words2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "def tokenizer(x):\n",
    "    return ( w for w in word_tokenize(x) if len(w) >3)\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    corpus = [sentence1, sentence2]\n",
    "\n",
    "    # Преобразуем предложения в матрицу TF-IDF\n",
    "    vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Вычисляем косинусное расстояние между предложениями\n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def word_similarity(word1, word2):\n",
    "    corpus = [word1, word2]\n",
    "\n",
    "    # Преобразуем слова в матрицу TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Вычисляем косинусное расстояние между словами\n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_similarity(text, texts):\n",
    "    dif = 0.1\n",
    "    c = 0\n",
    "    length = len(text)\n",
    "    similarity = 0.0\n",
    "    for other_text in texts:\n",
    "        other_length = len(other_text)\n",
    "        if other_length >= length * (1 + dif):\n",
    "            break\n",
    "        if other_length >= length * (1 - dif):\n",
    "            c += 1\n",
    "            current_similarity = sentence_similarity(text, other_text)\n",
    "            if current_similarity > similarity and current_similarity <= (1 - 0.1 ** 2):\n",
    "                    print(current_similarity, other_text)\n",
    "                    print(\"~\" * 80)\n",
    "                    similarity = current_similarity\n",
    "            # similarity = max(similarity, current_similarity)\n",
    "    print(c)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''Большой современный ewfwefwfwefwf ресторан в Москве. Отличное место. Красивый интерьер. \n",
    "Большой выбор блюд. Хороший сервис, официант всегда может помочь с выбором блюд. \n",
    "feefef кутабы с efefef, зеленью и тыквой.  wefweff всегда на высоте. Вот только ffff по сравнению с другими ресторанами уступает, маленькая порция, с рыбой не советую совсем слишком зажарена(буквально три маленьких кусочка)\n",
    "Отмечали здесь не один большой праздник, всегда все довольны. Все ваши капризы как wefwefw за ваш счет. \n",
    "Летом efefef efefef с видом на старый Арбат'''\n",
    "\n",
    "print(find_max_similarity(txt, sorted_texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
